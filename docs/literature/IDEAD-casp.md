## üîç **1. Project Idea & Scope**

**Goal**: Multimodal trajectory and behavior prediction
**Input**:

- Rasterized BEV (multi-camera, known poses)
- Static map (e.g., centerlines, boundaries)
- Dynamic context (agent states over past frames)
**Output**:
- Multimodal **vectorized trajectories**
- Optional: behavior class, maneuver type, textual descriptions
- Uncertainty modeling (e.g., Laplace)

---

## üß† **2. Core Architecture: CASPFormer (+CASPNet Backbone)**

### üì• Input Representation

- **Static Context**: Rasterized HD map (centerlines, crossings, etc.)
  - Shape: `(H, W, 3)`, e.g., `152 √ó 96 √ó 3`
- **Dynamic Context**: Rasterized history over `T_i = 3` time steps
  - Shape: `(T_i, H, W, C)`, e.g., `3 √ó 152 √ó 96 √ó 7`
  - Channels: One-hot actor type, offset `(Œ¥_u, Œ¥_v)`, velocity components `(v_u, v_v)`

---

### üß± Backbone (CASPNet)

**Purpose**: Encode static + dynamic scene into **multi-scale scene encodings**

```mermaid
graph TD
A[Static Map Raster (1x RGB image)] --> B[Map Encoder: Gabor Conv + CNN]
C[Dynamic Raster Series (Ti x 7)] --> D[Trajectory Encoder: Shared CNN]
D --> E[ConvLSTM (per-pixel temporal fusion)]
B --> F[Multi-Scale Feature Pyramid]
E --> F
F --> G[Scene Encodings (multi-resolution, multi-channel)]
```

#### Key components

- **CNN Blocks**: Per-frame extraction (shared weights across Ti)
- **ConvLSTM**: Aggregates temporal info across input frames
- **Attention Block**: Applies per-pixel dilation-aware filtering (e.g., slow pedestrians vs. fast cars)
- **Skip Connections**: Enable multi-scale fusion of map and trajectory encodings

üì¶ **Output**: Multi-scale scene features
e.g., `F‚ÇÅ: 76√ó48√ó64`, `F‚ÇÇ: 38√ó24√ó128`, ...

---

### üîÑ Recurrent Decoder (CASPFormer)

- Uses **deformable attention** on scene encodings
- Learns:
  - **Temporal Queries** (capture trajectory time correlation)
  - **Mode Queries** (ensure diverse trajectory modes)
- Uses a **recurrent loop**:
  - Updates reference points ‚Üí refines predictions iteratively
- Each mode: Predicts a sequence of `T_o = 12` steps (6s at 2 Hz)

---

## ‚öôÔ∏è **3. Training Details & Losses**

### üéØ Classification Loss (Cross Entropy)

- For mode selection `œÄ(k)`
- Softmax over all modes
- Encourages accurate selection of best mode
- Not hard argmax; uses soft distribution based on final point distance

### üìâ Regression Loss (Laplace-based NLL)

- Predicts `(Œº_{k,t}, b_{k,t})` for each mode/time
- Loss computed for **best matching mode** only
- Encourages accurate trajectory shape + uncertainty modeling

### üß† Entropy Loss (optional extension)

- Could regularize diversity between modes (prevent collapse)

---

## üß™ **4. Implementation & Compute**

- **Resolution**: `152m √ó 96m @ 1m grid`
- **Modes**: `M = 5` trajectories per agent
- **Training**: AdamW, A100 GPU
- **Augmentation**: Rotation ¬±60¬∞, Translation ¬±3m
- **Compute Cost**: Decoder very expensive (~60% of training)

---

## üìö **5. Datasets Overview**

| Dataset        | Focus                    | Notes |
|----------------|---------------------------|-------|
| **nuScenes**   | Urban, HD maps, rich sensors | 1000 scenes, 6s future, full 360¬∞ |
| Argoverse 1/2  | Rich map & 3D agents         | Good for interaction modeling |
| Waymo OMD      | Large-scale, 3D trajectories | Good quality but heavy |
| Lyft Level 5   | Multimodal, behavior labels | Prototype level use |
| INTERACTION    | Complex intersection behavior | Good for challenging multi-agent scenarios |
| ETH/UCY        | Pedestrian campus scenes    | Good for social prediction |
| SDD            | Aerial crowded scenes       | Good for group interaction |
| JAAD           | Joint attention cues        | Older, not SoTA |

---

## üß© **6. Extensions & Considerations**

- **Replace RNN**: Consider pure attention or temporal convolutions
- **Incorporate behavior**: Add maneuver classification or behavior prediction
- **Use vectorized objects**: With GNNs (e.g., PyTorch Geometric)
- **Knowledge Graphs**: Integrate nuScenes Knowledge Graph (e.g., SemanticFormer)
- **Diffusion Models**: Consider probabilistic decoders (e.g., DiffTraj)
- **Crowd Flow**: Extend to multi-view or high-level flow estimation
- **Simulation-based RL**: Use predictions in environments like Duckietown for agent control

---

## ‚ùå Limitations

| Type | Limitation |
|------|------------|
| **Temporal** | Only 1s history, no long-term modeling |
| **Representation** | Rasterization loses precision; vector or hybrid better |
| **Decoder** | Very compute-heavy |
| **Static Map** | Limited to road/lane info; lacks traffic light/sign modeling |
| **Trajectory Extraction** | Requires complex post-processing (e.g., NMS, interpolation) |
