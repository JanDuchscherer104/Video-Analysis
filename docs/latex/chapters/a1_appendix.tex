% Appendix
\appendix
\section{Appendix}

\subsection{Notation and Symbol Reference}
\label{app:notation}

This section provides a comprehensive reference for all symbols and conventions used throughout this work, following UniTraj framework standards~\cite{unitrajFeng2024} and the \textsc{DatasetItem} type definitions. The following tables have been created using~\cite{copilotSonnet}, given the type and shape definitions in \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/datasets/types.py}{unitraj/datasets/types.py}.

\begin{table}[H]
\caption{Temporal dimension symbols and definitions}
\centering
\begin{tabular}{p{3cm}p{10cm}}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
\(T_p, T_{\text{in}}\) & Number of past timesteps (historical context) \\
\(T_f, T_{\text{out}}\) & Number of future timesteps (prediction horizon) \\
\(T\) & Total trajectory length: \(T = T_p + T_f\) \\
\(\Delta t\) & Temporal sampling interval (i.e.\ 0.1s for 10Hz data) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Spatial and agent dimension symbols}
\label{tab:shape_symbols}
\centering
\begin{tabular}{p{3cm}p{10cm}}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
\(N_{\max}\) & Maximum number of agents in scene (i.e.\ 64) \\
\(N, N_A\) & Actual number of agents in a specific scenario \\
\(N_c\) & Number of center agents, \(N_c = N \) in \cite{lmformerYadav2025}. \\
\(K_{\max}\) & Maximum number of map polylines (i.e.\ 256) \\
\(K\) & Actual number of map polylines in a specific scenario \\
\(L\) & Number of points per map polyline (i.e.\ 20) \\
\( N_L\) & Number of lane segments in \( \boldsymbol{X}_s, N_L=K \cdot L \) \\
\(F_{ap}\) & Agent feature dimension (i.e.\ 39) \\
\(F_{af}\) & Agent future state dimension (4: \(x, y, v_x, v_y\)) \\
\(F_{map}\) & Map feature dimension (i.e.\ 29) \\
\(D\) & Hidden dimension of encodings or latent representations. \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Primary data tensors and their shapes}
\label{tab:data_tensors}
\centering
\begin{tabular}{p{4cm}p{9cm}}
\toprule
\textbf{Tensor} & \textbf{Shape and Description} \\
\midrule
\(\boldsymbol{X}_d\) & \([N_{\max}, T_p, F_{ap}]\) Agent trajectory features \\
\(\boldsymbol{M}_d\) & \([N_{\max}, T_p]\) Agent trajectory validity mask \\
\(\boldsymbol{X}_{d,pos}\) & \([N_{\max}, T_p, 3]\) Agent positions \((x, y, z)\) \\
\(\boldsymbol{X}_{d,last}\) & \([N_{\max}, 3]\) Last observed agent positions \\
\(\tilde{\boldsymbol{X}}_d\) & \([N_{\max}, T_f, F_{af}]\) Agent future states \\
\(\tilde{\boldsymbol{M}}_d\) & \([N_{\max}, T_f]\) Agent future validity mask \\
\(\boldsymbol{y}_c\) & \([T_f, F_{af}]\) Center agent ground truth trajectory \\
\(\boldsymbol{M}_c\) & \([T_f]\) Center agent ground truth validity mask \\
\(\boldsymbol{X}_s\) & \([K_{\max}, L, F_{map}]\) Map polyline features \\
\(\boldsymbol{M}_s\) & \([K_{\max}, L]\) Map polyline validity mask \\
\(\boldsymbol{C}_s\) & \([K_{\max}, 3]\) Map polyline centers \\
\bottomrule
\end{tabular}
\end{table}

Here, \( \tilde{\bullet}_{\circ} \) denotes tensors, expressing future states, while \(\bullet_{\circ}\) denotes tensors for historical or static context. \( \bullet_{d} \) denotes all tensors, representing \emph{dynamic} agents, while \(\bullet_{s}\) denotes tensors for \emph{static} map elements. The center agent, whose trajectory is the target for prediction, is denoted by the subscript \(c\).

\begin{table}[H]
\caption{Agent feature components ($F_{ap} = 39$)}
\centering
\begin{tabular}{p{3cm}p{3cm}p{7cm}}
\toprule
\textbf{Component} & \textbf{Indices} & \textbf{Description} \\
\midrule
Spatial & [0:6] & Position \((x, y, z)\), bbox-dimensions \((l, w, h)\) \\
Type & [6:11] & One-hot agent type encoding as per \ref{tab:agent_types}\\
Temporal & [11:33] & One-hot time embedding (\(T_p + 1\) dimensions) \\
Heading & [33:35] & Heading encoding \((\sin\theta, \cos\theta)\) \\
Kinematic & [35:39] & Velocity \((v_x, v_y)\), acceleration \((a_x, a_y)\) \\
\bottomrule
\end{tabular}
\end{table}

The utilized type encodings are reflecting the respective entity types, which are provided by ScenarioNet~\cite{scenarionetLi2023}, which uses MetaDrive~\cite{metadriveLi2022} internally. It should be noted that the use of these type encodings comes at the loss of more fine-grained semantic annotations that are provided by the original Argoverse2 dataset.

\begin{table}[H]
\caption{Map feature components ($F_{map} = 29$)}
\centering
\begin{tabular}{p{3cm}p{3cm}p{7cm}}
\toprule
\textbf{Component} & \textbf{Indices} & \textbf{Description} \\
\midrule
Position & [0:3] & Current point coordinates \((x, y, z)\) \\
Direction & [3:6] & Direction vector \((d_x, d_y, d_z)\) \\
Previous & [6:9] & Previous point coordinates \((x_{prev}, y_{prev}, z_{prev})\) \\
Lane Type & [9:29] & One-hot lane type encoding (20 categories) \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
    \caption{Agent type encodings}
    \label{tab:agent_types}
\centering
\begin{tabular}{p{3cm}p{2cm}p{8cm}}
\toprule
\textbf{Type} & \textbf{ID} & \textbf{Description} \\
\midrule
UNSET & 0 & Unknown or unclassified agent \\
VEHICLE & 1 & Cars, trucks, buses, motorcycles \\
PEDESTRIAN & 2 & Pedestrians, wheelchair users \\
CYCLIST & 3 & Bicyclists, e-scooter riders \\
\bottomrule
\end{tabular}
\end{table}

The PolylineType enumeration defines the mapping from MetaDrive/ScenarioNet polyline types to integer labels used in the \(F_{\text{map}}\) feature encoding. This enumeration is derived from the MetaDrive simulation environment~\cite{metadriveLi2022} and ScenarioNet dataset format~\cite{scenarionetLi2023}.

\begin{table}[H]
\centering
\caption{PolylineType Enumeration as per MetaDrive}
\label{tab:polyline-types}
\begin{tabular}{p{2cm}p{11cm}}
\toprule
\textbf{Integer} & \textbf{Description} \\
\midrule
0 & Default/unspecified polyline type \\
1 & Highway or freeway lane \\
2 & Surface street lane \\
3 & Dedicated bicycle lane \\
6 & Broken single white line \\
7 & Solid single white line \\
8 & Solid double white line \\
9 & Broken single yellow line \\
10 & Broken double yellow line \\
11 & Solid single yellow line \\
12 & Solid double yellow line \\
13 & Passing zone double yellow line \\
15 & Road boundary line \\
16 & Median boundary \\
17 & Stop sign location \\
18 & Pedestrian crosswalk \\
19 & Speed bump or traffic calming \\
\bottomrule
\end{tabular}
\end{table}

Note that the conversion from AV2 \( \rightarrow \) ScenarioNet \( \rightarrow \) UniTraj causes many categories to collapse, and plenty of the categories as per \autoref{tab:polyline-types} and  \autoref{tab:agent_types} are actually used, and hence result in degenerate encodings.


\subsubsection{Dataset Metadata}

\begin{table}[h]
\centering
\caption{UniTraj Sample Metadata Fields}
\label{tab:sample_metadata_fields}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lp{8cm}l@{}}
\toprule
\textbf{Field} & \textbf{Description} & \textbf{Data Type} \\
\midrule
\texttt{h5\_path} & Path to the HDF5 file containing the processed sample data & Path \\
\texttt{scenario\_id} & Original scenario identifier from ScenarioNet & str \\
\texttt{kalman\_difficulty} & Array of Kalman filter difficulty scores for all agents in the scenario & np.ndarray \\
\texttt{num\_agents} & Total number of agents with valid trajectories in the scenario & int64 \\
\texttt{num\_agents\_interest} & Number of agents of interest (prediction candidates) in the scenario & int64 \\
\texttt{scenario\_future\_duration} & Number of future timesteps available in the scenario & int64 \\
\texttt{num\_map\_polylines} & Total number of map polylines in the scenario & int64 \\
\texttt{track\_index\_to\_predict} & Index of the specific agent track to predict within the scenario & int64 \\
\texttt{center\_objects\_type} & Semantic type of the centered object (vehicle, pedestrian, cyclist) & category \\
\texttt{dataset\_name} & Name of the source dataset (e.g., av2\_scenarionet) & category \\
\texttt{trajectory\_type} & Behavioral classification of the trajectory & category \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsection{Deformable Attention}
\label{sec:deformable_attention}

Traditional self and cross-attention in transformers~\cite{vaswani2023attention} scales quadratically with spatial resolution \( \mathcal{O}(H^2W^2) \)\footnote{Note that the sequence consists of \(H \times W\) image patches or activations, and not the original image resolution.}, making it computationally infeasible for high-resolution feature maps. Deformable attention~\cite{zhu2021deformabledetr} addresses this by attending to a sparse set of sampling locations \(\{\mathbf{p}_q + \boldsymbol{\Delta p}_{nqk}\}_{k=1}^{K_s}\) around each query position \(\mathbf{p}_q\), where \(\boldsymbol{\Delta p}_{nqk}\) are dynamically computed \emph{fractional} offsets and \(K_s\) is the number of sampling points per head. This reduces the complexity to \(\mathcal{O}(NHK_s)\)\footnote{Simplified for single head attention}.

For a query with content features \(\mathbf{Z}_q\) with reference point \(\mathbf{p}_q \in [0, 1]^2\) and an input feature map \( \mathbf{X} \) multi-head deformable attention with \( N_h \) heads computes:
\begin{equation}
  \label{eq:deformable_attention_general}
  \text{DeformAttn}(\mathbf{Z}_q, \mathbf{X}, \mathbf{p}_q) = \sum_{n=1}^{N_{H}} \mathbf{W}_n \left[ \sum_{k=1}^{K_s} A_{nkq} \cdot \mathbf{X}(\mathbf{p}_q + \boldsymbol{\Delta p}_{nkq}) \right]
\end{equation}
where \(\mathbf{W}_m\) are learned projection matrices and \(\mathbf{X}(\bullet)\) represents bilinear sampling from the feature map \( \mathbf{X} \in \R^{C \times H \times W} \). Both offsets \(\boldsymbol{\Delta p}_{nkq}\) and attention weights \(A_{nkq}\) are computed via linear projections of the query and key features:
\begin{equation}
\label{eq:deformable_attention_offsets}
\tilde{\bullet} \propto \text{activ}(\underbrace{\mathbf{Z}^T_q \mathbf{U}_n^T}_{\mathbf{Q}}\underbrace{\mathbf{V}_n \mathbf{X}_k}_{\mathbf{K}}) \quad \text{with activ} = \begin{cases}
 &\text{softmax} \text{ for }\tilde{\bullet} = A_{nkq} \\
 &\text{sigmoid} \text{ for }\tilde{\bullet} = \boldsymbol{\Delta p}_{nkq}
\end{cases}
\end{equation}

\( \mathbf{U}_n \) and \( \mathbf{V}_n \) are learned projection matrices to compute the queries \(\mathbf{Q} \) and \emph{keys} \(\mathbf{K}\) in the \( n \)-th attention head. In case of \emph{self}-attention, all the queries, keys, and values are derived from the same sequence, i.e. \( \mathbf{Z}_q = \mathbf{X} \), while in case of \emph{cross}-attention, the queries stem from a different sequence than the keys and values. In a simpler single-head attention setting the keys are computed as \( \mathbf{K} = \mathbf{X}\mathbf{W} \).

\paragraph{Conceptual Interpretation.} To understand deformable attention, it helps to conceptualize the key components~\cite{copilotSonnet}:

\begin{description}[leftmargin=1em,itemsep=3pt]
\item[Queries (\(\mathbf{Q}\)).] Represent \emph{what information is being sought}. In trajectory prediction, queries encode the current prediction state (temporal context and mode embeddings) that determines what spatial features to attend to for generating the next trajectory point.

\item[Keys (\(\mathbf{K}\)) and Values (\(\mathbf{V}\)).] Keys act as \emph{indices} that determine the compatibility between queries and spatial locations, while values contain the \emph{actual feature content} to be aggregated. In CASPFormer, both are derived from the multi-scale CNN feature maps, encoding spatial scene context at different resolutions.

\item[Reference Points (\(\mathbf{p}_q\)).] Serve as \emph{anchor locations} in normalized coordinates \([0,1]^{2}\) that ground the attention mechanism spatially. They represent the current prediction focus (e.g., the last predicted trajectory point) and provide spatial context for where to look in the feature maps.

\item[Sampling Offsets (\(\boldsymbol{\Delta p}_{nkq}\)).] Learned \emph{displacement vectors} that adaptively shift attention away from the reference point toward informative spatial locations. These enable the model to dynamically focus on relevant scene elements (e.g., lane boundaries, nearby agents) rather than being constrained to a fixed grid.

\item[Attention Weights (\(A_{nkq}\)).] Determine the \emph{importance} of each sampled location, allowing the model to emphasize the most relevant spatial features while suppressing irrelevant background information. The weights sum to 1 across all sampling points, creating a normalized spatial attention distribution.
\end{description}

The key innovation is that both offsets and weights are \emph{content-dependent}—computed dynamically based on the current query state rather than being fixed. This enables efficient sparse attention that adapts to the spatial structure of each specific scene, focusing computational resources on the most informative regions.

\subsection{Gabor Filter Steerability}
\label{ssec:gabor_filters}
Gabor filters are defined by a sinusoidal plane wave modulated by a Gaussian envelope:
\begin{equation}
\label{eq:gabor_filter}
G_{\lambda,\theta,\psi,\sigma,\gamma}(x,y)
= \exp\!\Bigl(-\tfrac{x'^{2} + \gamma^{2}y'^{2}}{2\sigma^{2}}\Bigr)
  \cos\!\Bigl(2\pi \tfrac{x'}{\lambda} + \psi\Bigr),
\end{equation}
where
\begin{equation}
\label{eq:gabor_rotation}
x' = x\cos\theta + y\sin\theta,\quad
y' = -x\sin\theta + y\cos\theta,
\end{equation}
and \(\lambda\) (wavelength), \(\theta\) (orientation), \(\psi\) (phase), \(\sigma\) (scale), and \(\gamma\) (aspect ratio) control the filter's frequency and spatial extent~\cite{Luan2018GCNN}.

Crucially, Gabor filters are \emph{steerable}, meaning any rotated version can be expressed as a linear combination of a finite set of basis filters:
\begin{equation}
\label{eq:gabor_steerability}
G_{\lambda,\theta}(x,y)
= \sum_{n=1}^N k_n(\theta)\,G_{\lambda,\theta_n}(x,y),
\end{equation}
with fixed prototype orientations \(\{\theta_n\}\) and interpolation coefficients \(\{k_n(\theta)\}\)~\cite{steerableGaborFilters}. This property enables the network to capture directional patterns without having to learn separate filters for each orientation—leading to improved sample efficiency and built-in \(\mathrm{SO}(2)\)-equivariance. In CASPNet, this steerability is leveraged to induce rotational quasi-equivariance in the first layers of the map encoder, ensuring that road elements are represented robustly regardless of global scene orientation. Additionally, by using filter banks that span multiple scales (\(\sigma\)) and aspect ratios (\(\gamma\)), the architecture gains partial \(\mathrm{SIM}(2)\)-equivariance, responding consistently to rescaled and rotated map structures~\cite{steerableGaborFilters}.

\subsection{The UniTraj Dataprocessing Pipeline}
\label{app:framework}

The pseudo-code in this section are mostly generated using~\cite{copilotSonnet}.

\textbf{Phase 1: Temporal Window Extraction}
The first processing stage extracts \emph{uniformly sampled} windows containing historical context (\(T_p\) steps) and future ground truth (\(T_f\) steps) from raw agent trajectories. Frequency masking ensures consistent uniform temporal resolution with a sampling interval \(\Delta t_{s}\) across datasets, and is combined with the original validity masks to indicate which observations are valid at each timestep. Validity masks indicate missing observations throughout the pipeline.\\

\begin{algorithm}[H]
\caption{Phase 1: Temporal Window Extraction}
\label{alg:phase1_temporal}
\begin{algorithmic}[1]
\REQUIRE Raw scenario tracks \(\mathcal{T}\), time horizons \(T_p, T_f\), sampling interval \(\Delta t_{s}\)
\ENSURE Temporally windowed agent trajectories with validity masks
\STATE \(T_{total} \leftarrow T_p + T_f\) \COMMENT{Total time window length}
\STATE \(M_{freq} \leftarrow \text{generate\_mask}(T_p - 1, T_{total}, \Delta t_{s})\) \COMMENT{Temporal sampling mask}
\FOR{each agent track \(i \in \mathcal{T}\), timestep \(t = 0\) to \(T_{total}\)}
    \STATE Extract state vectors: \(\boldsymbol{s}_i^{(t)} = [\boldsymbol{p}_i^{(t)}, l_i, w_i, h_i, \theta_i^{(t)}, \boldsymbol{v}_i^{(t)}, \text{valid}_i^{(t)}]^T\)
    \STATE Apply temporal windowing: \(\boldsymbol{s}_i \leftarrow \boldsymbol{s}_i[t_{start} : t_{start} + T_{total}]\)
    \STATE Apply frequency masking: \(\text{valid}_i^{(t)} \leftarrow \text{valid}_i^{(t)} \cdot M_{freq}[t]\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 2: Map Feature Processing}

This phase converts heterogeneous map primitives (lanes, boundaries, signs, crosswalks) into standardized polyline sequences with consistent geometric and semantic encoding. Polyline interpolation ensures uniform point density, direction vectors encode the local orientation of each segment, and type-based filtering selects relevant map elements for prediction scenarios.

\begin{algorithm}[H]
\caption{Phase 2: Map Feature Processing}
\label{alg:phase2_map}
\begin{algorithmic}[1]
\REQUIRE Raw map data \(\mathcal{M}\), interpolation distance \(d_{interp}\)
\ENSURE Standardized map polylines with geometric and semantic features
\FOR{each map element \(m \in \mathcal{M}\)}
    \STATE Interpolate polyline points with uniform spacing \(d_{interp}\)
    \STATE Compute direction vectors: \(\boldsymbol{d}_{k,l} = \boldsymbol{p}_{k,l+1} - \boldsymbol{p}_{k,l}\)
    \STATE Assign semantic type encoding: \(\boldsymbol{o}_{type} \in \{0,1\}^{20}\)
    \STATE Store polyline: \(\boldsymbol{L}_k = [\boldsymbol{p}_{k,l}, \boldsymbol{d}_{k,l}, \boldsymbol{o}_{type}]_{l=1}^{L}\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 3: Agent Selection and Filtering}

Agent filtering ensures that only relevant trajectories with sufficient motion and observation quality are retained for training. This phase applies distance-based motion thresholds, present-time validity requirements, and future continuity constraints to identify suitable center agents.

\begin{algorithm}[H]
\caption{Phase 3: Agent Selection and Filtering}
\label{alg:phase3_filtering}
\begin{algorithmic}[1]
\REQUIRE Agent trajectories \(\mathcal{A}\), motion threshold \(d_{min} = 2.0\)m
\ENSURE Filtered set of center agents \(\mathcal{A}_{center}\)
\STATE \(\mathcal{A}_{center} \leftarrow \emptyset\)
\FOR{each agent \(i \in \mathcal{A}\)}
    \STATE Compute total motion: \(\Delta d_i = \sum_{t=1}^{T_p-1} \|\boldsymbol{p}_i^{(t)} - \boldsymbol{p}_i^{(t-1)}\|_2\)
    \IF{\(\Delta d_i \geq d_{min}\) AND \(\text{valid}_i^{(T_p-1)} = 1\)}
        \STATE Add to center agents: \(\mathcal{A}_{center} \leftarrow \mathcal{A}_{center} \cup \{i\}\)
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 4: Coordinate System Transformation}

For each center agent, the entire scene (including all other agents and map elements) is transformed to an agent-centric coordinate frame. This transformation consists of translation to center the agent's position at \(t = T_p - 1\) at the origin, followed by rotation to align the agent's heading with the positive \(x\)-axis.

\begin{algorithm}[H]
\caption{Phase 4: Coordinate System Transformation}
\label{alg:phase4_transform}
\begin{algorithmic}[1]
\REQUIRE Center agent \(c\), all trajectories \(\mathcal{A}\), map polylines \(\mathcal{L}\)
\ENSURE Agent-centric coordinates for all elements
\STATE \(\boldsymbol{p}_c^{ref} \leftarrow \boldsymbol{p}_c^{(T_p-1)}\) \COMMENT{Reference position}
\STATE \(\theta_c^{ref} \leftarrow \theta_c^{(T_p-1)}\) \COMMENT{Reference heading}
\FOR{each agent \(i \in \mathcal{A}\), timestep \(t\)}
    \STATE Translate: \(\boldsymbol{p}_i^{(t)} \leftarrow \boldsymbol{p}_i^{(t)} - \boldsymbol{p}_c^{ref}\)
    \STATE Rotate: \(\boldsymbol{p}_i^{(t)} \leftarrow \mathbf{R}(-\theta_c^{ref}) \boldsymbol{p}_i^{(t)}\)
    \STATE Transform heading: \(\theta_i^{(t)} \leftarrow \theta_i^{(t)} - \theta_c^{ref}\)
    \STATE Rotate velocity: \(\boldsymbol{v}_i^{(t)} \leftarrow \mathbf{R}(-\theta_c^{ref}) \boldsymbol{v}_i^{(t)}\)
\ENDFOR
\FOR{each polyline \(k \in \mathcal{L}\), point \(l\)}
    \STATE Apply same coordinate transformation to polyline points
    \STATE Filter polylines within spatial range: \(\|\boldsymbol{p}_{polyline}\|_2 \leq \text{map\_range}\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

After this transformation, all spatial map features and agent states are expressed in a common reference frame, whose origin is the center agent's position at the last historical timestep \(T_p-1\). This yields both \emph{translation} and \emph{rotation} invariance, when perceiving the scene from the center agent's perspective.

\textbf{Phase 5: Feature Vector Assembly}

This phase constructs the full feature vectors for each agent by concatenating spatial state, agent type encoding, temporal step embedding, heading representation, and kinematic features. Invalid trajectory points are zeroed out according to the validity masks.

\begin{algorithm}[H]
\caption{Phase 5: Feature Vector Assembly}
\label{alg:phase5_features}
\begin{algorithmic}[1]
\REQUIRE Transformed agent states, validity masks
\ENSURE Complete agent feature vectors \(\boldsymbol{X}_d \in \mathbb{R}^{N_{\max} \times T_p \times F_{ap}}\)
\FOR{each agent \(i\), timestep \(t\)}
    \STATE Spatial features: \(\boldsymbol{f}_{spatial} = [\boldsymbol{p}_i^{(t)}, l_i, w_i, h_i] \in \mathbb{R}^6\)
    \STATE Type encoding: \(\boldsymbol{o}_{type} \in \{0,1\}^5\)
    \STATE Time embedding: \(\boldsymbol{e}_{time} \in \{0,1\}^{T_p+1}\)
    \STATE Heading encoding: \(\boldsymbol{h}_{embed} = [\sin(\theta_i^{(t)}), \cos(\theta_i^{(t)})] \in \mathbb{R}^2\)
    \STATE Kinematic features: \(\boldsymbol{f}_{kinematic} = [\boldsymbol{v}_i^{(t)}, \boldsymbol{a}_i^{(t)}] \in \mathbb{R}^4\)
    \STATE Concatenate: \(\boldsymbol{X}_d^{(i,t)} = [\boldsymbol{f}_{spatial}, \boldsymbol{o}_{type}, \boldsymbol{e}_{time}, \boldsymbol{h}_{embed}, \boldsymbol{f}_{kinematic}]\)
    \IF{\(\text{valid}_i^{(t)} = 0\)}
        \STATE \(\boldsymbol{X}_d^{(i,t)} \leftarrow \boldsymbol{0}\)
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 6: Agent Proximity Filtering and Padding}

To ensure computational tractability, the pipeline retains only the \(N_{\max}\) closest agents to each center agent, where proximity is measured by the Euclidean distance at the final historical timestep. Agent features are then zero-padded to the fixed dimension \(N_{\max}\) to enable efficient batch processing.

\begin{algorithm}[H]
\caption{Phase 6: Agent Proximity Filtering and Padding}
\label{alg:phase6_proximity}
\begin{algorithmic}[1]
\REQUIRE Agent features \(\boldsymbol{X}_d\), maximum agents \(N_{\max} = 64\)
\ENSURE Padded agent tensor \(\boldsymbol{X}_d \in \mathbb{R}^{N_{\max} \times T_p \times F_{ap}}\)
\FOR{each center agent \(c\)}
    \STATE Compute distances: \(d_{ic} = \|\boldsymbol{p}_i^{(T_p-1)} - \boldsymbol{p}_c^{(T_p-1)}\|_2\) for all agents \(i\)
    \STATE Select top-\(N_{\max}\) closest agents by distance
    \STATE Zero-pad features to dimension \(N_{\max} \times T_p \times F_{ap}\)
    \STATE Create validity mask: \(\boldsymbol{M}_d \in \{0,1\}^{N_{\max} \times T_p}\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 7: Map Feature Processing}

The agent-centric map polylines are converted into a fixed-size tensor representations in a three-stage process: segmentation based on geometric discontinuities (gaps \(>\) 1.0m), uniform resampling to exactly \(L\) points per segment, and proximity-based selection of the top \(K_{\max}\) segments closest to the center agent. Each point is encoded with geometric context including position, direction vectors, previous point reference, and semantic type information. The resulting map tensor \(\boldsymbol{X}_s\) is padded to a fixed size of \(K_{\max}\) segments, each with \(L\) points, and a validity mask \(\boldsymbol{M}_s\) is created to indicate which segments are actually present.

\begin{algorithm}[H]
\caption{Phase 7: Map Feature Processing}
\label{alg:phase7_map_features}
\begin{algorithmic}[1]
\REQUIRE Agent-centric polylines, max polylines \(K_{\max} = 256\), points per polyline \(L = 20\)
\ENSURE Map tensor \(\boldsymbol{X}_s \in \mathbb{R}^{K_{\max} \times L \times F_{map}}\), validity mask \(\boldsymbol{M}_s\)
\FOR{each polyline \(k\)}
    \STATE Segment polyline at geometric discontinuities (gaps \(> 1.0\)m)
    \STATE Resample each segment to exactly \(L\) uniform points
    \STATE Compute geometric features: position, direction, previous point
    \STATE Append semantic type encoding: \(\boldsymbol{o}_{type} \in \{0,1\}^{20}\)
    \STATE Assemble: \(\boldsymbol{X}_s^{(k,l)} = [\text{position}, \text{direction}, \text{previous}, \boldsymbol{o}_{type}]\)
\ENDFOR
\STATE Select top-\(K_{\max}\) polylines by proximity to center agent
\STATE Zero-pad to fixed dimensions and create validity mask
\end{algorithmic}
\end{algorithm}

\textbf{Phase 8: Future Trajectory Processing}

This phase processes the future trajectory ground truth for each agent and creates the center agent's target trajectory. Future trajectories are extracted, transformed to agent-centric coordinates, and validity masks are created to handle variable-length future observations.

\begin{algorithm}[H]
\caption{Phase 8: Future Trajectory Processing}
\label{alg:phase8_future}
\begin{algorithmic}[1]
\REQUIRE Future trajectory data, center agent indices
\ENSURE Center ground truth \(\boldsymbol{y}_c \in \mathbb{R}^{T_f \times 4}\), validity masks
\FOR{each agent \(i\), future timestep \(t \in [T_p, T_p + T_f)\)}
    \STATE Extract future state: \(\boldsymbol{s}_i^{(t)} = [\boldsymbol{p}_i^{(t)}, \boldsymbol{v}_i^{(t)}]\)
    \STATE Apply agent-centric transformation
    \STATE Store in future tensor: \(\tilde{\boldsymbol{X}}_d[i, t-T_p] = \boldsymbol{s}_i^{(t)}\)
\ENDFOR
\FOR{each center agent \(c\)}
    \STATE Extract center ground truth: \(\boldsymbol{y}_c = \tilde{\boldsymbol{X}}_d[c, :, :]\)
    \STATE Create future validity mask: \(\tilde{\boldsymbol{M}}_d \in \{0,1\}^{T_f}\)
    \STATE Compute final valid index: \(idx_{final} = \max\{t : \tilde{\boldsymbol{M}}_d[t] = 1\}\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 9: DatasetItem Assembly}

The final processing phase assembles all processed components into final \texttt{DatasetItem} instances. It performs data validation, applies optional attribute masking (e.g., zeroing z-coordinates or object bounding boxes), ensures float32 compatibility, and finally creates structured \texttt{DatasetItem} instances.
\begin{algorithm}[H]
\caption{Phase 9: DatasetItem Assembly}
\label{alg:phase9_assembly}
\begin{algorithmic}[1]
\REQUIRE All processed tensors and masks
\ENSURE Final \texttt{DatasetItem} instance
\STATE Create \texttt{DatasetItem} instance with all processed tensors
\STATE Apply attribute masking (optional): zero out z-axis, size, velocity, etc.
\STATE Convert floating data types to float32
\RETURN \texttt{DatasetItem} instance
\end{algorithmic}
\end{algorithm}

The resulting \texttt{DatasetItem}'s are subsequently saved to disk for training and evaluation. They include all agent tensors, map tensors, ground truth labels, and validity masks as \texttt{numpy} arrays in various formats for easy handling at different training, evaluation or visualization stages.
The \texttt{DatasetItem} class serves as the primary data container for processed trajectory scenarios. Figure~\ref{fig:datasetitem_class} shows the complete structure of this class, including all tensor attributes, metadata fields, and utility methods used throughout the training and evaluation pipeline.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/classes_DataSetItem.png}
\caption{Class diagram of the \texttt{DatasetItem} structure showing all tensor attributes, metadata fields, and utility methods. This class encapsulates the processed scenario data as described in \autoref{app:framework}.}
\label{fig:datasetitem_class}
\end{figure}

The \texttt{DatasetItem} contains the standardized tensor representations described in \autoref{tab:data_tensors}, including agent trajectories (\texttt{obj\_trajs}), map polylines (\texttt{map\_polylines}), ground truth targets (\texttt{center\_gt\_trajs}), and their corresponding validity masks. The resulting \texttt{DatasetItem}'s are subsequently saved to disk for training and evaluation. They include all agent tensors, map tensors, ground truth labels, and validity masks as \texttt{numpy} arrays in various formats for easy handling at different training, evaluation or visualization stages.

Figure~\ref{fig:litdatamodule_class} illustrates the PyTorch Lightning data module architecture used for batch processing and data loading. The \texttt{LitDatamodule} and its configuration class manage the entire data pipeline from raw scenarios to training-ready batches.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/classes_LitDatmodule.png}
\caption{Class diagram of the data processing components showing the \texttt{LitDatamodule} and \texttt{LitDatamoduleConfig} classes. These components orchestrate the data loading pipeline described in \autoref{app:framework}.}
\label{fig:litdatamodule_class}
\end{figure}

The \texttt{LitDatamodule} implements the PyTorch Lightning data module interface, providing standardized train, validation, and test data loaders with configurable batch sizes, worker processes, and memory pinning options as specified in the \texttt{LitDatamoduleConfig}.


\subsection{UniTraj Directory Structure}
\label{app:directory_structure}

\dirtree{%
.1 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/}{\texttt{unitraj/}}.
.2 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/__init__.py}{\texttt{\_\_init\_\_.py}}.
.2 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/configs/}{\texttt{configs/}}.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/configs/experiment_config.py}{\texttt{experiment\_config.py}}.
.4 \texttt{ExperimentConfig} ― top-level experiment config.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/configs/path_config.py}{\texttt{path\_config.py}}.
.4 \texttt{PathConfig} ― centralized Singleton for path-handling.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/configs/wandb_config.py}{\texttt{wandb\_config.py}}.
.4 \texttt{WandBConfig} ― WandB integration for PyTorch Lightning.
.2 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/datasets/}{\texttt{datasets/}}.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/datasets/base_dataparser.py}{\texttt{base\_dataparser.py}}.
.4 \texttt{BaseDataParser} - multiprocessing, file \& metadata handling.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/datasets/dataparser.py}{\texttt{dataparser.py}}.
.4 \texttt{DataParser} ― Processing pipeline as per~\autoref{app:framework}.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/datasets/base_dataset.py}{\texttt{base\_dataset.py}} ― \texttt{BaseDataset} wrapper.
.4 \texttt{BaseDataset} ― PyTorch dataset.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/datasets/common_utils.py}{\texttt{common\_utils.py}} ― shared parsers \& transforms.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/datasets/types.py}{\texttt{types.py}} ― type definitions.
.4 \texttt{DatasetItem} ― final dataset item structure.
.4 \texttt{BatchInputDict} ― collated tensor dict.
.2 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/lightning/}{\texttt{lightning/}}.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/lightning/lit_datamodule.py}{\texttt{lit\_datamodule.py}} ― \texttt{LightningDataModule}.
.4 \texttt{LitDatamodule} ― PyTorch Lightning data module.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/lightning/lit_trainer_factory.py}{\texttt{lit\_trainer\_factory.py}} ― \texttt{pl.Trainer} factory.
.4 \texttt{TrainerFactory} ― factory for \texttt{pl.Trainer}.
.2 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/models/}{\texttt{models/}}.
.3 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/models/base_model/base_model.py}{\texttt{base\_model.py}} ― \texttt{BaseModel}, \texttt{BaseModelConfig}.
.4 \texttt{BaseModel} ― base \texttt{pl.LightningModule} for UniTraj.
.2 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/utils/console.py}{\texttt{console.py}}.
.3 \texttt{Console} ― Rich console for logging.
.2 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/utils/base_config.py}{\texttt{base\_config.py}}.
.3 \texttt{BaseConfig} ― abstract base config for Config-as-Factory pattern.
.2 \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/utils/visualization.py}{\texttt{visualization.py}} ― plotting and rendering helpers.
.3 \texttt{plot\_dataset\_item()} ― static visualization for single DatasetItem.
.3 \texttt{check\_loaded\_data()} ― data integrity visualization check.
.3 \texttt{visualize\_batch\_data()} ― batch data visualization.
.3 \texttt{concatenate\_images()} ― image concatenation utility.
.3 \texttt{concatenate\_varying()} ― varying size image concatenation.
.3 \texttt{visualize\_prediction()} ― prediction result visualization.
}
