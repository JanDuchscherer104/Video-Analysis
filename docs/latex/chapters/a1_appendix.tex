% Appendix
\appendix
\section{Appendix}

\subsection{The UniTraj Dataprocessing Pipeline}

\textbf{Phase 1: Temporal Window Extraction}

The first processing stage extracts \emph{uniformly sampled} windows containing historical context (\(T_p\) steps) and future ground truth (\(T_f\) steps) from raw agent trajectories. Frequency masking ensures consistent uniform temporal resolution with a sampling interval \(\Delta t_{s}\) across datasets, and is combined with the original validity masks to indicate which observations are valid at each timestep. Validity masks indicate missing observations throughout the pipeline.

\begin{algorithm}[H]
\caption{Phase 1: Temporal Window Extraction}
\label{alg:phase1_temporal}
\begin{algorithmic}[1]
\REQUIRE Raw scenario tracks \(\mathcal{T}\), time horizons \(T_p, T_f\), sampling interval \(\Delta t_{s}\)
\ENSURE Temporally windowed agent trajectories with validity masks
\STATE \(T_{total} \leftarrow T_p + T_f\) \COMMENT{Total time window length}
\STATE \(M_{freq} \leftarrow \text{generate\_mask}(T_p - 1, T_{total}, \Delta t_{s})\) \COMMENT{Temporal sampling mask}
\FOR{each agent track \(i \in \mathcal{T}\), timestep \(t = 0\) to \(T_{total}\)}
    \STATE Extract state vectors: \(\boldsymbol{s}_i^{(t)} = [\boldsymbol{p}_i^{(t)}, l_i, w_i, h_i, \theta_i^{(t)}, \boldsymbol{v}_i^{(t)}, \text{valid}_i^{(t)}]^T\)
    \STATE Apply temporal windowing: \(\boldsymbol{s}_i \leftarrow \boldsymbol{s}_i[t_{start} : t_{start} + T_{total}]\)
    \STATE Apply frequency masking: \(\text{valid}_i^{(t)} \leftarrow \text{valid}_i^{(t)} \cdot M_{freq}[t]\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 2: Map Feature Processing}

This phase converts heterogeneous map primitives (lanes, boundaries, signs, crosswalks) into standardized polyline sequences with consistent geometric and semantic encoding. Polyline interpolation ensures uniform point density, direction vectors encode the local orientation of each segment, and type-based filtering selects relevant map elements for prediction scenarios.

\begin{algorithm}[H]
\caption{Phase 2: Map Feature Processing}
\label{alg:phase2_map}
\begin{algorithmic}[1]
\REQUIRE Raw map data \(\mathcal{M}\), interpolation distance \(d_{interp}\)
\ENSURE Standardized map polylines with geometric and semantic features
\FOR{each map element \(m \in \mathcal{M}\)}
    \STATE Interpolate polyline points with uniform spacing \(d_{interp}\)
    \STATE Compute direction vectors: \(\boldsymbol{d}_{k,l} = \boldsymbol{p}_{k,l+1} - \boldsymbol{p}_{k,l}\)
    \STATE Assign semantic type encoding: \(\boldsymbol{o}_{type} \in \{0,1\}^{20}\)
    \STATE Store polyline: \(\boldsymbol{L}_k = [\boldsymbol{p}_{k,l}, \boldsymbol{d}_{k,l}, \boldsymbol{o}_{type}]_{l=1}^{L}\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 3: Agent Selection and Filtering}

Agent filtering ensures that only relevant trajectories with sufficient motion and observation quality are retained for training. This phase applies distance-based motion thresholds, present-time validity requirements, and future continuity constraints to identify suitable center agents.

\begin{algorithm}[H]
\caption{Phase 3: Agent Selection and Filtering}
\label{alg:phase3_filtering}
\begin{algorithmic}[1]
\REQUIRE Agent trajectories \(\mathcal{A}\), motion threshold \(d_{min} = 2.0\)m
\ENSURE Filtered set of center agents \(\mathcal{A}_{center}\)
\STATE \(\mathcal{A}_{center} \leftarrow \emptyset\)
\FOR{each agent \(i \in \mathcal{A}\)}
    \STATE Compute total motion: \(\Delta d_i = \sum_{t=1}^{T_p-1} \|\boldsymbol{p}_i^{(t)} - \boldsymbol{p}_i^{(t-1)}\|_2\)
    \IF{\(\Delta d_i \geq d_{min}\) AND \(\text{valid}_i^{(T_p-1)} = 1\)}
        \STATE Add to center agents: \(\mathcal{A}_{center} \leftarrow \mathcal{A}_{center} \cup \{i\}\)
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 4: Coordinate System Transformation}

For each center agent, the entire scene (including all other agents and map elements) is transformed to an agent-centric coordinate frame. This transformation consists of translation to center the agent's position at \(t = T_p - 1\) at the origin, followed by rotation to align the agent's heading with the positive \(x\)-axis.

\begin{algorithm}[H]
\caption{Phase 4: Coordinate System Transformation}
\label{alg:phase4_transform}
\begin{algorithmic}[1]
\REQUIRE Center agent \(c\), all trajectories \(\mathcal{A}\), map polylines \(\mathcal{L}\)
\ENSURE Agent-centric coordinates for all elements
\STATE \(\boldsymbol{p}_c^{ref} \leftarrow \boldsymbol{p}_c^{(T_p-1)}\) \COMMENT{Reference position}
\STATE \(\theta_c^{ref} \leftarrow \theta_c^{(T_p-1)}\) \COMMENT{Reference heading}
\FOR{each agent \(i \in \mathcal{A}\), timestep \(t\)}
    \STATE Translate: \(\boldsymbol{p}_i^{(t)} \leftarrow \boldsymbol{p}_i^{(t)} - \boldsymbol{p}_c^{ref}\)
    \STATE Rotate: \(\boldsymbol{p}_i^{(t)} \leftarrow \mathbf{R}(-\theta_c^{ref}) \boldsymbol{p}_i^{(t)}\)
    \STATE Transform heading: \(\theta_i^{(t)} \leftarrow \theta_i^{(t)} - \theta_c^{ref}\)
    \STATE Rotate velocity: \(\boldsymbol{v}_i^{(t)} \leftarrow \mathbf{R}(-\theta_c^{ref}) \boldsymbol{v}_i^{(t)}\)
\ENDFOR
\FOR{each polyline \(k \in \mathcal{L}\), point \(l\)}
    \STATE Apply same coordinate transformation to polyline points
    \STATE Filter polylines within spatial range: \(\|\boldsymbol{p}_{polyline}\|_2 \leq \text{map\_range}\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

After this transformation, all spatial map features and agent states are expressed in a common reference frame, whose origin is the center agent's position at the last historical timestep \(T_p-1\). This yields both \emph{translation} and \emph{rotation} invariance, when perceiving the scene from the center agent's perspective.

\textbf{Phase 5: Feature Vector Assembly}

This phase constructs the full feature vectors for each agent by concatenating spatial state, agent type encoding, temporal step embedding, heading representation, and kinematic features. Invalid trajectory points are zeroed out according to the validity masks.

\begin{algorithm}[H]
\caption{Phase 5: Feature Vector Assembly}
\label{alg:phase5_features}
\begin{algorithmic}[1]
\REQUIRE Transformed agent states, validity masks
\ENSURE Complete agent feature vectors \(\boldsymbol{X}_d \in \mathbb{R}^{N_{\max} \times T_p \times F_{ap}}\)
\FOR{each agent \(i\), timestep \(t\)}
    \STATE Spatial features: \(\boldsymbol{f}_{spatial} = [\boldsymbol{p}_i^{(t)}, l_i, w_i, h_i] \in \mathbb{R}^6\)
    \STATE Type encoding: \(\boldsymbol{o}_{type} \in \{0,1\}^5\)
    \STATE Time embedding: \(\boldsymbol{e}_{time} \in \{0,1\}^{T_p+1}\)
    \STATE Heading encoding: \(\boldsymbol{h}_{embed} = [\sin(\theta_i^{(t)}), \cos(\theta_i^{(t)})] \in \mathbb{R}^2\)
    \STATE Kinematic features: \(\boldsymbol{f}_{kinematic} = [\boldsymbol{v}_i^{(t)}, \boldsymbol{a}_i^{(t)}] \in \mathbb{R}^4\)
    \STATE Concatenate: \(\boldsymbol{X}_d^{(i,t)} = [\boldsymbol{f}_{spatial}, \boldsymbol{o}_{type}, \boldsymbol{e}_{time}, \boldsymbol{h}_{embed}, \boldsymbol{f}_{kinematic}]\)
    \IF{\(\text{valid}_i^{(t)} = 0\)}
        \STATE \(\boldsymbol{X}_d^{(i,t)} \leftarrow \boldsymbol{0}\)
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 6: Agent Proximity Filtering and Padding}

To ensure computational tractability, the pipeline retains only the \(N_{\max}\) closest agents to each center agent, where proximity is measured by the Euclidean distance at the final historical timestep. Agent features are then zero-padded to the fixed dimension \(N_{\max}\) to enable efficient batch processing.

\begin{algorithm}[H]
\caption{Phase 6: Agent Proximity Filtering and Padding}
\label{alg:phase6_proximity}
\begin{algorithmic}[1]
\REQUIRE Agent features \(\boldsymbol{X}_d\), maximum agents \(N_{\max} = 64\)
\ENSURE Padded agent tensor \(\boldsymbol{X}_d \in \mathbb{R}^{N_{\max} \times T_p \times F_{ap}}\)
\FOR{each center agent \(c\)}
    \STATE Compute distances: \(d_{ic} = \|\boldsymbol{p}_i^{(T_p-1)} - \boldsymbol{p}_c^{(T_p-1)}\|_2\) for all agents \(i\)
    \STATE Select top-\(N_{\max}\) closest agents by distance
    \STATE Zero-pad features to dimension \(N_{\max} \times T_p \times F_{ap}\)
    \STATE Create validity mask: \(\boldsymbol{M}_d \in \{0,1\}^{N_{\max} \times T_p}\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 7: Map Feature Processing}

The agent-centric map polylines are converted into a fixed-size tensor representations in a three-stage process: segmentation based on geometric discontinuities (gaps \(>\) 1.0m), uniform resampling to exactly \(L\) points per segment, and proximity-based selection of the top \(K_{\max}\) segments closest to the center agent. Each point is encoded with geometric context including position, direction vectors, previous point reference, and semantic type information. The resulting map tensor \(\boldsymbol{X}_s\) is padded to a fixed size of \(K_{\max}\) segments, each with \(L\) points, and a validity mask \(\boldsymbol{M}_s\) is created to indicate which segments are actually present.

\begin{algorithm}[H]
\caption{Phase 7: Map Feature Processing}
\label{alg:phase7_map_features}
\begin{algorithmic}[1]
\REQUIRE Agent-centric polylines, max polylines \(K_{\max} = 256\), points per polyline \(L = 20\)
\ENSURE Map tensor \(\boldsymbol{X}_s \in \mathbb{R}^{K_{\max} \times L \times F_{map}}\), validity mask \(\boldsymbol{M}_s\)
\FOR{each polyline \(k\)}
    \STATE Segment polyline at geometric discontinuities (gaps \(> 1.0\)m)
    \STATE Resample each segment to exactly \(L\) uniform points
    \STATE Compute geometric features: position, direction, previous point
    \STATE Append semantic type encoding: \(\boldsymbol{o}_{type} \in \{0,1\}^{20}\)
    \STATE Assemble: \(\boldsymbol{X}_s^{(k,l)} = [\text{position}, \text{direction}, \text{previous}, \boldsymbol{o}_{type}]\)
\ENDFOR
\STATE Select top-\(K_{\max}\) polylines by proximity to center agent
\STATE Zero-pad to fixed dimensions and create validity mask
\end{algorithmic}
\end{algorithm}

\textbf{Phase 8: Future Trajectory Processing}

This phase processes the future trajectory ground truth for each agent and creates the center agent's target trajectory. Future trajectories are extracted, transformed to agent-centric coordinates, and validity masks are created to handle variable-length future observations.

\begin{algorithm}[H]
\caption{Phase 8: Future Trajectory Processing}
\label{alg:phase8_future}
\begin{algorithmic}[1]
\REQUIRE Future trajectory data, center agent indices
\ENSURE Center ground truth \(\boldsymbol{y}_c \in \mathbb{R}^{T_f \times 4}\), validity masks
\FOR{each agent \(i\), future timestep \(t \in [T_p, T_p + T_f)\)}
    \STATE Extract future state: \(\boldsymbol{s}_i^{(t)} = [\boldsymbol{p}_i^{(t)}, \boldsymbol{v}_i^{(t)}]\)
    \STATE Apply agent-centric transformation
    \STATE Store in future tensor: \(\tilde{\boldsymbol{X}}_d[i, t-T_p] = \boldsymbol{s}_i^{(t)}\)
\ENDFOR
\FOR{each center agent \(c\)}
    \STATE Extract center ground truth: \(\boldsymbol{y}_c = \tilde{\boldsymbol{X}}_d[c, :, :]\)
    \STATE Create future validity mask: \(\tilde{\boldsymbol{M}}_d \in \{0,1\}^{T_f}\)
    \STATE Compute final valid index: \(idx_{final} = \max\{t : \tilde{\boldsymbol{M}}_d[t] = 1\}\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Phase 9: DatasetItem Assembly}

The final processing phase assembles all processed components into final \texttt{DatasetItem} instances. It performs data validation, applies optional attribute masking (e.g., zeroing z-coordinates or object bounding boxes), ensures float32 compatibility, and finally creates structured \texttt{DatasetItem} instances.
\begin{algorithm}[H]
\caption{Phase 9: DatasetItem Assembly}
\label{alg:phase9_assembly}
\begin{algorithmic}[1]
\REQUIRE All processed tensors and masks
\ENSURE Final \texttt{DatasetItem} instance
\STATE Create \texttt{DatasetItem} instance with all processed tensors
\STATE Apply attribute masking (optional): zero out z-axis, size, velocity, etc.
\STATE Convert floating data types to float32
\RETURN \texttt{DatasetItem} instance
\end{algorithmic}
\end{algorithm}

The resulting \texttt{DatasetItem}'s are subsequently saved to disk for training and evaluation. They include all agent tensors, map tensors, ground truth labels, and validity masks as \texttt{numpy} arrays in various formats for easy handling at different training, evaluation or visualization stages.

\subsection{Notation and Symbol Reference}
\label{app:notation}

This section provides a comprehensive reference for all symbols and conventions used throughout this work, following UniTraj framework standards~\cite{unitrajFeng2024} and the \textsc{DatasetItem} type definitions. The following tables have been created using \cite{githubCopilot2021} and \cite{claude35Sonnet2024}, given the type and shape definitions in \href{https://github.com/JanDuchscherer104/UniTraj/blob/main/unitraj/datasets/types.py}{unitraj/datasets/types.py}.

\begin{table}[H]
\caption{Temporal dimension symbols and definitions}
\centering
\begin{tabular}{p{3cm}p{10cm}}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
\(T_p\) & Number of past timesteps (historical context) \\
\(T_f\) & Number of future timesteps (prediction horizon) \\
\(T\) & Total trajectory length: \(T = T_p + T_f\) \\
\(\Delta t\) & Temporal sampling interval (i.e.\ 0.1s for 10Hz data) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Spatial and agent dimension symbols}
\label{tab:shape_symbols}
\centering
\begin{tabular}{p{3cm}p{10cm}}
\toprule
\textbf{Symbol} & \textbf{Definition} \\
\midrule
\(N_{\max}\) & Maximum number of agents in scene (i.e.\ 64) \\
\(N\) & Actual number of agents in a specific scenario \\
\(N_c\) & Number of center agents (i.e.\ 1) \\
\(K_{\max}\) & Maximum number of map polylines (i.e.\ 256) \\
\(K\) & Actual number of map polylines in a specific scenario \\
\(L\) & Number of points per map polyline (i.e.\ 20) \\
\(F_{ap}\) & Agent feature dimension (i.e.\ 39) \\
\(F_{af}\) & Agent future state dimension (4: \(x, y, v_x, v_y\)) \\
\(F_{map}\) & Map feature dimension (i.e.\ 29) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Primary data tensors and their shapes}
\label{tab:data_tensors}
\centering
\begin{tabular}{p{4cm}p{9cm}}
\toprule
\textbf{Tensor} & \textbf{Shape and Description} \\
\midrule
\(\boldsymbol{X}_d\) & \([N_{\max}, T_p, F_{ap}]\) Agent trajectory features \\
\(\boldsymbol{M}_d\) & \([N_{\max}, T_p]\) Agent trajectory validity mask \\
\(\boldsymbol{X}_{d,pos}\) & \([N_{\max}, T_p, 3]\) Agent positions \((x, y, z)\) \\
\(\boldsymbol{X}_{d,last}\) & \([N_{\max}, 3]\) Last observed agent positions \\
\(\tilde{\boldsymbol{X}}_d\) & \([N_{\max}, T_f, F_{af}]\) Agent future states \\
\(\tilde{\boldsymbol{M}}_d\) & \([N_{\max}, T_f]\) Agent future validity mask \\
\(\boldsymbol{y}_c\) & \([T_f, F_{af}]\) Center agent ground truth trajectory \\
\(\boldsymbol{M}_c\) & \([T_f]\) Center agent ground truth validity mask \\
\(\boldsymbol{X}_s\) & \([K_{\max}, L, F_{map}]\) Map polyline features \\
\(\boldsymbol{M}_s\) & \([K_{\max}, L]\) Map polyline validity mask \\
\(\boldsymbol{C}_s\) & \([K_{\max}, 3]\) Map polyline centers \\
\bottomrule
\end{tabular}
\end{table}

Here, \( \tilde{\bullet}_{\circ} \) denotes tensors, expressing future states, while \(\bullet_{\circ}\) denotes tensors for historical or static context. \( \bullet_{d} \) denotes all tensors, representing \emph{dynamic} agents, while \(\bullet_{s}\) denotes tensors for \emph{static} map elements. The center agent, whose trajectory is the target for prediction, is denoted by the subscript \(c\).

\begin{table}[H]
\caption{Agent feature components ($F_{ap} = 39$)}
\centering
\begin{tabular}{p{3cm}p{3cm}p{7cm}}
\toprule
\textbf{Component} & \textbf{Indices} & \textbf{Description} \\
\midrule
Spatial & [0:6] & Position \((x, y, z)\), bbox-dimensions \((l, w, h)\) \\
Type & [6:11] & One-hot agent type encoding as per \ref{tab:agent_types}\\
Temporal & [11:33] & One-hot time embedding (\(T_p + 1\) dimensions) \\
Heading & [33:35] & Heading encoding \((\sin\theta, \cos\theta)\) \\
Kinematic & [35:39] & Velocity \((v_x, v_y)\), acceleration \((a_x, a_y)\) \\
\bottomrule
\end{tabular}
\end{table}

The utilized type encodings are reflecting the respective entity types, which are provided by ScenarioNet~\cite{scenarionetFeng2024}, which uses MetaDrive~\cite{metadriveLi2022} internally. It should be noted that the use of these type encodings comes at the loss of more fine-grained semantic annotations that are provided by the original Argoverse2 dataset.

\begin{table}[H]
\caption{Map feature components ($F_{map} = 29$)}
\centering
\begin{tabular}{p{3cm}p{3cm}p{7cm}}
\toprule
\textbf{Component} & \textbf{Indices} & \textbf{Description} \\
\midrule
Position & [0:3] & Current point coordinates \((x, y, z)\) \\
Direction & [3:6] & Direction vector \((d_x, d_y, d_z)\) \\
Previous & [6:9] & Previous point coordinates \((x_{prev}, y_{prev}, z_{prev})\) \\
Lane Type & [9:29] & One-hot lane type encoding (20 categories) \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
    \caption{Agent type encodings}
    \label{tab:agent_types}
\centering
\begin{tabular}{p{3cm}p{2cm}p{8cm}}
\toprule
\textbf{Type} & \textbf{ID} & \textbf{Description} \\
\midrule
UNSET & 0 & Unknown or unclassified agent \\
VEHICLE & 1 & Cars, trucks, buses, motorcycles \\
PEDESTRIAN & 2 & Pedestrians, wheelchair users \\
CYCLIST & 3 & Bicyclists, e-scooter riders \\
\bottomrule
\end{tabular}
\end{table}

The PolylineType enumeration defines the mapping from MetaDrive/ScenarioNet polyline types to integer labels used in the \(F_{\text{map}}\) feature encoding. This enumeration is derived from the MetaDrive simulation environment~\cite{metadriveLi2022} and ScenarioNet dataset format~\cite{scenarionetLi2023}.

\begin{table}[H]
\centering
\caption{PolylineType Enumeration as per MetaDrive}
\label{tab:polyline-types}
\begin{tabular}{p{2cm}p{11cm}}
\toprule
\textbf{Integer} & \textbf{Description} \\
\midrule
0 & Default/unspecified polyline type \\
1 & Highway or freeway lane \\
2 & Surface street lane \\
3 & Dedicated bicycle lane \\
6 & Broken single white line \\
7 & Solid single white line \\
8 & Solid double white line \\
9 & Broken single yellow line \\
10 & Broken double yellow line \\
11 & Solid single yellow line \\
12 & Solid double yellow line \\
13 & Passing zone double yellow line \\
15 & Road boundary line \\
16 & Median boundary \\
17 & Stop sign location \\
18 & Pedestrian crosswalk \\
19 & Speed bump or traffic calming \\
\bottomrule
\end{tabular}
\end{table}

Note that the conversion from AV2 \( \rightarrow \) ScenarioNet \( \rightarrow \) UniTraj causes many categories to collapse, and plenty of the categories as per \autoref{tab:polyline-types} and  \autoref{tab:agent_types} are actually used, and hence result in degenerate encodings.